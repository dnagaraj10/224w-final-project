{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 2: Molecular Classification using Graph-Level Features\n",
        "In this notebook, we train two machine learning models simultaneously: one on the graph-level features and one on the graph structure. The model trained on the Graph Sturcture is a Graph Attention Network (GAT) that receives a Laplacian Matrix and generates a graph embedding. We combine the graph embeddings generated from these two strategies via a weight parameter. During each training step, we compute loss and perform a backward pass on both models. Adapted for demonstration purposes, this code uses a subset of the data in order to speed up training."
      ],
      "metadata": {
        "id": "c_vuqHKw4eFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6tmlbepwQ0U",
        "outputId": "4ce080ea-cade-4f25-eec5-fcc2daa7e87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/224W Final Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "FOLDERNAME = \"224W Final Project\"\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Packages and Data"
      ],
      "metadata": {
        "id": "O_Toyl5o4qdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5pNN2OiswNy",
        "outputId": "ad513a2d-ddc3-413f-c1f4-3774c75a5edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 05:38:44--  http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2-train.sdf.tar.gz\n",
            "Resolving ogb-data.stanford.edu (ogb-data.stanford.edu)... 171.64.75.57\n",
            "Connecting to ogb-data.stanford.edu (ogb-data.stanford.edu)|171.64.75.57|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1559712928 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘pcqm4m-v2-train.sdf.tar.gz.22’\n",
            "\n",
            "pcqm4m-v2-train.sdf 100%[===================>]   1.45G  24.1MB/s    in 53s     \n",
            "\n",
            "2023-12-15 05:39:36 (28.2 MB/s) - ‘pcqm4m-v2-train.sdf.tar.gz.22’ saved [1559712928/1559712928]\n",
            "\n",
            "fd72bce606e7ddf36c2a832badeec6ab  pcqm4m-v2-train.sdf.tar.gz\n"
          ]
        }
      ],
      "source": [
        "! wget http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2-train.sdf.tar.gz\n",
        "! md5sum pcqm4m-v2-train.sdf.tar.gz # fd72bce606e7ddf36c2a832badeec6ab\n",
        "! tar -xf pcqm4m-v2-train.sdf.tar.gz # extracted pcqm4m-v2-train.sdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XALt-we0t0IA",
        "outputId": "1885e574-6b2e-456a-9b43-8633fbf6a27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.2\n"
          ]
        }
      ],
      "source": [
        "! pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m9OoSP8ltH2v"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "\n",
        "suppl = Chem.SDMolSupplier('pcqm4m-v2-train.sdf')\n",
        "# for idx, mol in enumerate(suppl):\n",
        "#     print(f'{idx}-th rdkit mol obj: {mol}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfjtEEHvvvDc",
        "outputId": "1215dec6-e5de-49cd-e501-eb8d715c3525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/78.8 kB\u001b[0m \u001b[31m992.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=c61502e5340bc74cf1305c301f5867016a60d0f7da8fefce029d11a251ff9ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "! pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvDYhulqtZj6",
        "outputId": "449984a5-c0f6-4d4b-f5df-9ac43100ffce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('CC(NCC[C@H]([C@@H]1CCC(=CC1)C)C)C', 6.811009678015001)\n"
          ]
        }
      ],
      "source": [
        "from ogb.lsc import PCQM4Mv2Dataset\n",
        "dataset = PCQM4Mv2Dataset(root = FOLDERNAME, only_smiles = True)\n",
        "\n",
        "#See one molecule in the dataset\n",
        "i = 1234\n",
        "print(dataset[i]) # ('CC(NCC[C@H]([C@@H]1CCC(=CC1)C)C)C', 6.811009678015001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T7_2JXwB5fA",
        "outputId": "dde3ee58-433d-44a5-9815-d52fbfd00e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euQG8tEhfOYT"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pukFvBEvFT1K"
      },
      "outputs": [],
      "source": [
        "from ogb.utils import smiles2graph\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvXUUdRVj3tY"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZp-2b5WfbuA"
      },
      "outputs": [],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data: Generate Graphs from the strings that represent molecules.\n",
        "For example, the string `\"O=C1[N]c2ccncc2[CH][C@@H]1c1ccc(cc1)C\"`represents a molecule."
      ],
      "metadata": {
        "id": "DIx1TVGe6uBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj_pmYsjn8EL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import rdmolops\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#Converting the smiles strings to graphs\n",
        "\n",
        "#dataset: a PCQM4Mv2Dataset of strings\n",
        "print(dataset[0]) # ('O=C1[N]c2ccncc2[CH][C@@H]1c1ccc(cc1)C', 3.0476751256)\n",
        "desired_size = 4000 # A smaller dataset trains faster\n",
        "smaller_dataset = list(dataset)[:desired_size]\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import rdmolops\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    try:\n",
        "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
        "        AllChem.MMFFOptimizeMolecule(mol)\n",
        "    except ValueError:  # handles unregcognized charge states\n",
        "        return None\n",
        "\n",
        "    adj = rdmolops.GetAdjacencyMatrix(mol)\n",
        "    features = torch.tensor(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=512), dtype=torch.float32)\n",
        "    nonzero_indices = np.array(adj.nonzero(), dtype=np.long).T\n",
        "    edge_index_tensor = torch.tensor(nonzero_indices, dtype=torch.long).t().contiguous()\n",
        "    return {\"x\": features, \"edge_index\": edge_index_tensor}\n",
        "\n",
        "graph_data = [(smiles_to_graph(data), torch.tensor(label, dtype=torch.float32)) for data, label in smaller_dataset if smiles_to_graph(data) is not None]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKojn6DfZJCu"
      },
      "outputs": [],
      "source": [
        "#Finding the median label so that we can decide the dividing line between soluble and insoluble\n",
        "label_tensors = [data[1] for data in graph_data]\n",
        "all_labels = torch.stack(label_tensors)\n",
        "median_value = torch.median(all_labels).item()\n",
        "\n",
        "print(\"Median Label:\", median_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate train_loader and valid_loader from the data"
      ],
      "metadata": {
        "id": "-Di3exq57Sf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxiRlq_4ZcrA"
      },
      "outputs": [],
      "source": [
        "#Loading the Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, int):\n",
        "            # If idx is a scalar, return a single item\n",
        "            graph_item = self.data[idx]\n",
        "            x = graph_item[0]['x']\n",
        "            edge_index = graph_item[0]['edge_index']\n",
        "            label = graph_item[1]\n",
        "            return {'x': x, 'edge_index': edge_index}, label\n",
        "        elif isinstance(idx, (list, np.ndarray)):\n",
        "            # If idx is a list or numpy array, return a batch of items\n",
        "            batch_data = [self.data[i] for i in idx]\n",
        "            x_batch = [item[0]['x'] for item in batch_data]\n",
        "            edge_index_batch = [item[0]['edge_index'] for item in batch_data]\n",
        "            label_batch = [item[1] for item in batch_data]\n",
        "            data_dicts = [{'x': x, 'edge_index': edge_index} for x, edge_index in zip(x_batch, edge_index_batch)]\n",
        "            # Zip data_dicts with label_batch and return a list of tuples\n",
        "            data_tuples = list(zip(data_dicts, label_batch))\n",
        "            return data_tuples\n",
        "        else:\n",
        "            raise TypeError(\"Unsupported index type\")\n",
        "        return {'x': x, 'edge_index': edge_index}, label\n",
        "    def get_idx_split(self, test_size=0.2, random_state=None):\n",
        "        # Assuming self.data is a list\n",
        "        indices = list(range(len(self.data)))\n",
        "\n",
        "        # Split the indices into training and validation sets\n",
        "        train_indices, valid_indices = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        # Return a dictionary with keys \"train\" and \"valid\"\n",
        "        return {\"train\": train_indices, \"valid\": valid_indices}\n",
        "\n",
        "\n",
        "# Instantiate your dataset\n",
        "print(\"graph_data\", graph_data[0])\n",
        "graph_dataset = GraphDataset(graph_data)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 64  # You can adjust this based on your needs\n",
        "dataloader = DataLoader(graph_dataset, batch_size=batch_size, shuffle=True)\n",
        "split_idx = graph_dataset.get_idx_split()\n",
        "train_loader = DataLoader(graph_dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(graph_dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Model Applied to Graph Features\n",
        "This is a defintion of a classic machine learning model used to generate graph embeddings from graph features. It will the trained later, in conjunction with the Graph Attention Model (GAT)."
      ],
      "metadata": {
        "id": "5h7STJj870Dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE0aLYUF5qGH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "\n",
        "# Define the model to process the graph features. This model will be trained jointly\n",
        "# with a model trained on the graph structure later on in the colab.\n",
        "class GraphFeaturesModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_rate=0.5):\n",
        "        super(GraphFeaturesModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 512\n",
        "hidden_size = 256\n",
        "dropout_rate = 0.2\n",
        "model_graph_features = GraphFeaturesModel(input_size, hidden_size, dropout_rate)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_graph_features.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluting the Model\n",
        "This is the most important code block of the notebook. Here we train and evaluate both machine learning models: the Graph Attention Model for the graph structure and the classic one for the graph-level features. Each training step involves a forward pass and a backward pass for each model. The code below also presents the accuracy and loss curves, as well as the R2 score."
      ],
      "metadata": {
        "id": "CYAq7BjM9FTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE2V276Px-bb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_sparse import SparseTensor\n",
        "from sklearn.metrics import r2_score\n",
        "#This cell contains the most important code for this project.\n",
        "# It jointly trains the models for graph strcuture and graph features, as well\n",
        "# as the weight parameter that balances the resulting tensors. It also computes\n",
        "#accuracy over the epochs.\n",
        "\n",
        "\n",
        "# In this code block, we define the GAT model to train on a Laplacian matrix\n",
        "# that represents the node connectivity.\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads=1):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.gat = GATConv(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            heads=num_heads,\n",
        "            concat=True,\n",
        "            dropout=0.6\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm1d(out_features * num_heads)\n",
        "\n",
        "    def forward(self, x, edge_index, laplacian_matrix):\n",
        "        # Apply Laplacian matrix\n",
        "        lap_x = laplacian_matrix @ x\n",
        "\n",
        "        # GAT convolution\n",
        "        x = self.gat(lap_x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GraphNetGlobalPooling(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GraphNetGlobalPooling, self).__init__()\n",
        "        self.gat1 = GATLayer(input_size, hidden_size, num_heads=4)\n",
        "        self.gat2 = GATLayer(hidden_size * 4, output_size, num_heads=1)\n",
        "        self.weight = nn.Parameter(torch.Tensor(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        num_nodes = edge_index.max().item() + 1\n",
        "        x = model_graph_features(x)\n",
        "\n",
        "        #Creating Laplacian Matrix\n",
        "        adj_matrix = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(num_nodes, num_nodes))\n",
        "        deg_matrix = adj_matrix.sum(dim=1).to_dense()\n",
        "        laplacian_matrix = torch.diag(deg_matrix.squeeze()) - adj_matrix.to_dense()\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(laplacian_matrix, UPLO='U')\n",
        "        sorted_indices = torch.argsort(eigenvalues)\n",
        "        sorted_eigenvalues = eigenvalues[sorted_indices]\n",
        "        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        k = 5\n",
        "        top_k_eigenvectors = laplacian_matrix #sorted_eigenvectors[:, :]\n",
        "        #uncomment the sorted_eigenvectors part above to activate the SignNet-enhanced laplacian matrix\n",
        "\n",
        "\n",
        "        node_features = torch.randn([num_nodes, input_size])\n",
        "        x1 = self.gat1(node_features, edge_index, top_k_eigenvectors)\n",
        "        x2 = self.gat2(x1, edge_index, top_k_eigenvectors)\n",
        "        x2 = torch.mean(x2, dim=0, keepdim=True)\n",
        "\n",
        "        #Combining the two embeddings using the learned weight parameter\n",
        "        weighted_average = self.weight * x + (1 - self.weight) * x2\n",
        "        return weighted_average\n",
        "\n",
        "\n",
        "#Intializing the two models that will train together\n",
        "input_size = 512\n",
        "hidden_size = 64\n",
        "output_size = 1  # Regression task, single output\n",
        "model_graph_structure = GraphNetGlobalPooling(input_size, hidden_size, output_size)\n",
        "criterion_structure = nn.MSELoss()\n",
        "optimizer_structure = torch.optim.Adam(model_graph_structure.parameters(), lr=0.001)\n",
        "\n",
        "input_size = 512\n",
        "hidden_size = 256\n",
        "dropout_rate = 0.2\n",
        "model_graph_features = GraphFeaturesModel(input_size, hidden_size, dropout_rate)\n",
        "criterion_features = nn.MSELoss()\n",
        "optimizer_features = torch.optim.Adam(model_graph_features.parameters(), lr=0.001)\n",
        "losses = []\n",
        "training_accuracies = []\n",
        "val_losses = []\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    right_train = 0\n",
        "    total_train = 0\n",
        "    epoch_valid_loss = 0\n",
        "    for data, label in train_loader.dataset:\n",
        "        x = data['x']\n",
        "        #loss step for graph_structure model\n",
        "        label_pred = model_graph_structure(x, data['edge_index'])\n",
        "        loss = criterion_structure(label_pred, label.float())\n",
        "        optimizer_structure.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_structure.step()\n",
        "\n",
        "        # loss step for graph_features model\n",
        "        label_pred = model_graph_features(x)\n",
        "        loss = criterion_features(label_pred, label.float())\n",
        "        optimizer_features.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_features.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        #training accuracy\n",
        "        label_pred = model_graph_structure(x, data['edge_index'])\n",
        "        y_train_pred = 1 if label_pred > median_value else 0\n",
        "        y_train = 1 if label > median_value else 0\n",
        "        right_train += y_train == y_train_pred\n",
        "        total_train += 1\n",
        "\n",
        "    training_accuracy = right_train / total_train\n",
        "    training_accuracies.append(training_accuracy)\n",
        "    losses.append(epoch_loss / len(train_loader.dataset))\n",
        "    model_graph_structure.eval()\n",
        "    with torch.no_grad():\n",
        "      total = 0\n",
        "      right = 0\n",
        "      y_true_valid = []\n",
        "      y_pred_valid = []\n",
        "      for data, label in valid_loader.dataset:\n",
        "          x = data['x']\n",
        "          edge_index = data['edge_index']\n",
        "          #preparing for validation R2\n",
        "          label_pred = model_graph_structure(x, edge_index)\n",
        "          y_true_valid.append(label)\n",
        "          y_pred_valid.append(label_pred)\n",
        "\n",
        "          #validation accuracy\n",
        "          label_pred = 1 if label_pred[0][0] > median_value else 0\n",
        "          y_test = 1 if label > median_value else 0\n",
        "          total += 1\n",
        "          right += y_test == label_pred\n",
        "\n",
        "          #validation loss\n",
        "          label_pred = model_graph_features(data['x'])\n",
        "          loss = criterion_features(label_pred, label.float())\n",
        "          epoch_valid_loss += loss.item()\n",
        "      r2_valid = r2_score(y_true_valid, y_pred_valid)\n",
        "      print(f\"R^2 (Validation): {r2_valid}\")\n",
        "\n",
        "\n",
        "      val_losses.append(epoch_valid_loss/len(valid_loader.dataset))\n",
        "      accuracy = right / total\n",
        "      accuracies.append(accuracy)\n",
        "      print(\"Validation Accuracy:\", accuracy)\n",
        "      # Plot the loss curve\n",
        "      plt.plot(losses, label='Training Loss')\n",
        "      plt.plot(accuracies, label='Validation Accuracy')\n",
        "      plt.plot(val_losses, label='Validation Loss')\n",
        "      plt.plot(training_accuracies, label='Training Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss and Accuracy')\n",
        "      plt.title('Training Loss and Validation Accuracy Curve')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      # Evaluation loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw436gB8heZm"
      },
      "outputs": [],
      "source": [
        "print(\"accuracies\", accuracies)\n",
        "print(\"losses\", losses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}